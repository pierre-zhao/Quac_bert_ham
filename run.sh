python3 cqa_run_his_atten.py --output_dir=output_dir --max_considered_history_turns=4 --num_train_epochs=20.0 --train_steps=58000 --learning_rate=3e-5 --n_best_size=20 --better_hae=True --MTL=True --MTL_lambda=0.1 --MTL_mu=0.8 --train_batch_size=8  --predict_batch_size=8 --evaluate_after=50000 --evaluation_steps=1000 --fine_grained_attention=True --bert_hidden=1024 --max_answer_length=50 --max_seq_length=512 --load_small_portion=False --cache_dir=cache_large/ --bert_config_file=uncased_L-24_H-1024_A-16/bert_config.json --init_checkpoint=uncased_L-24_H-1024_A-16/bert_model.ckpt --vocab_file=uncased_L-24_H-1024_A-16/vocab.txt --mtl_input=reduce_mean --quac_train_file=train_v0.2.json  --quac_predict_file=val_v0.2.json --warmup_proportion=0.1 
